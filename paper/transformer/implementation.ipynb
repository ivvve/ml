{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install gdown\n",
    "%pip install transformers\n",
    "%pip install sentencepiece\n",
    "%pip install sacremoses\n",
    "%pip install einopsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"font\", family=\"AppleGothic\") # 한글이 안 깨지도록 font 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99220b5b8b504e21b774083bba705b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5e012774394441901d2d88ebdcbc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eos_idx=0, pad_idx=65000\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# tokenizer & input embedding layer & last fc layer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "\n",
    "eos_idx = tokenizer.eos_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "print(f\"{eos_idx=}, {pad_idx=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 논문에서는 2.5만 token이 한 batch에 담기게 했다고 함\n",
    "LAMBDA = 0 # for L2-Regularization\n",
    "EPOCH = 15\n",
    "\n",
    "# max_len = 512\n",
    "max_len = 100 # 512는 너무 길어서 잘랐다\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx) # label이 pad인 y는 무시한다, 언제 pad가 나오는지는 학습할 필요가 없음\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, label_smoothing=0.1) # label smoothing 성능이 안나옴\n",
    "\n",
    "# LR scheduler\n",
    "scheduler_name = \"Noam\"\n",
    "# scheduler_name = \"Cos\"\n",
    "#### Noam ####\n",
    "# warmup_steps = 4000 # 논문에서 제시한 값 (10만 step의 4%)\n",
    "warmup_steps = 1500 # 데이터 수 * epoch / bs = 총 step 수\n",
    "LR_scale = 0.5\n",
    "#### Cos ####\n",
    "LR_init = 5e-4\n",
    "T0 = 1500 # 첫 주기\n",
    "T_mult = 2\n",
    "\n",
    "new_model_train = False\n",
    "save_model_path = \"./results/Transformer_small2.pt\"\n",
    "save_history_path = \"./results/Transformer_small2_history.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=65001\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"{vocab_size=}\")\n",
    "\n",
    "# 논문에 나오는 모델\n",
    "# n_layers = 6\n",
    "# d_model = 512\n",
    "# d_ff = 2048\n",
    "# n_heads = 8\n",
    "# drop_p = 0.1\n",
    "\n",
    "# 사이즈 줄인 모델\n",
    "n_layers = 3\n",
    "d_model = 256\n",
    "d_ff = 512\n",
    "n_heads = 8\n",
    "drop_p = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer & pretrained model 써보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁H', 'i', ',', '▁I', \"'\", 'm', '▁Ch', 'ri', 's', '.', '▁', '.', '.', '▁', '.', '?']\n",
      "['▁ch', 'at', 'G', 'P', 'T']\n",
      "['▁p', 're', 'tra', 'in', 'ed', '▁re', 'st', 'art']\n",
      "['▁한', '글', '은', '▁어떻게', '▁할까', '?']\n",
      "['▁나는', '▁너는', '▁나의', '▁그녀와', '▁그것', '과']\n"
     ]
    }
   ],
   "source": [
    "# _는 띄어쓰기 (_art와 art는 다름)\n",
    "print(tokenizer.tokenize(\"Hi, I'm Chris. .. .?\"))\n",
    "print(tokenizer.tokenize(\"chatGPT\"))\n",
    "print(tokenizer.tokenize(\"pretrained restart\"))\n",
    "\n",
    "print(tokenizer.tokenize(\"한글은 어떻게 할까?\"))\n",
    "print(tokenizer.tokenize(\"나는 너는 나의 그녀와 그것과\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
